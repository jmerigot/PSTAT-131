---
title: "Homework 3"
author: "Jules Merigot (8488256)"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: pdf_document
---

```{r setup, echo=FALSE, include=FALSE}
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(ISLR)
library(ISLR2)
library(corrplot)
library(ggthemes)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR) # for naive bayes
library(finalfit)
library(pROC)
tidymodels_prefer()

library(knitr)
# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(fig.width=7, fig.height=5)
options(digits = 4)

## indents are for indenting r code as formatted text
## They may need to be adjusted depending on your OS
# if your output looks odd, increase or decrease indent
indent1 = '    '
indent2 = '        '
indent3 = '            '
```

```{r, results="hide"}
# loading the data
titanic_data <- read.csv(file = "C:/Users/jules/OneDrive/Desktop/homework-3/data/titanic.csv")
head(titanic_data)

titanic_data$survived <- factor(titanic_data$survived, labels = c("Yes", "No"))
titanic_data$pclass <- factor(titanic_data$pclass)

str(titanic_data)
```

# Classification

## Question 1

```{r}
# setting the seed
set.seed(8488)

titanic_split <- initial_split(titanic_data, prop=0.60, strata=survived)

titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
```
For splitting the data, I chose a proportion of 0.60 because it allows for more training data, while retaining enough data to be tested since there is a limited amount of observations. The training data has 534 observations while the testing data has 357 observations. 

```{r}
#plot of missing values in the training data
missing_plot(titanic_train)

# the number of missing values in the training data
sum(is.na(titanic_train))
```
There is a good amount of missing data in the training data, 530 missing data values to be exact, particularly for the *age* and *cabin* variables, as can be seen in the plot above.  
  
We want to use stratified sampling for this data because not only does it allow for less bias, but since we have less observations than the abalone dataset for example, stratified sampling allows for more precision on a smaller dataset, and thus a more precise sample in this case.

## Question 2

```{r}
titanic_train %>% 
  ggplot(aes(x = survived)) +
  geom_bar()
```
Using the above visualization of the distribution of the outcome variable *survived*, we can see that less people survived than people that perished on the Titanic. More than 300 (known) passengers lost their lives, while only a little more than 200 passengers survived.

## Question 3

```{r}
cor_titanic <- titanic_train %>%
  select(where(is.numeric)) %>%
  correlate()
rplot(cor_titanic)
```
After making the correlation matrix above, there are some clear patterns that emerge, such as most variables being slightly negatively correlated with others, with some exceptions. *parch* and *sib_sp* have a positive correlation, which means that the number of siblings/spouses of a certain passenger is positively correlated with the number of children/parents of that passenger, which makes sense. Additionally, *sib_sp* and *age* are negatively correlated, which indicates that a passenger's age is negatively correlated with the number of siblings/spouses they have. This makes sense because younger passengers will tend to travel alone, and thus are less likely to have siblings or spouses.

## Question 4

```{r}
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, 
                         data=titanic_train) %>%
  step_impute_linear(age, impute_with = imp_vars(all_predictors())) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(terms = ~ sex_male:fare + age:fare) 

titanic_recipe %>% prep() %>% juice()
```

## Question 5

```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_recipe)

log_fit <- fit(log_wkflow, titanic_train)

log_fit %>% 
  tidy()
```

## Question 6

```{r}
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titanic_recipe)

lda_fit <- fit(lda_wkflow, titanic_train)
```

## Question 7

```{r}
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)

qda_fit <- fit(qda_wkflow, titanic_train)
```

## Question 8

```{r}
nb_mod <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("klaR") %>% 
  set_args(usekernel = FALSE) 

nb_wkflow <- workflow() %>% 
  add_model(nb_mod) %>% 
  add_recipe(titanic_recipe)

nb_fit <- fit(nb_wkflow, titanic_train)
```

## Question 9

We now generate predictions of each of the four models using our training data, and we use the accuracy metric to asses their performance.
```{r, results="hide", warning=FALSE}
library(yardstick)

log_predict <- predict(log_fit, titanic_train) %>%
  bind_cols(titanic_train$survived) %>%
  accuracy(truth = titanic_train$survived, estimate = .pred_class)
log_predict

lda_predict <- predict(lda_fit, titanic_train) %>%
  bind_cols(titanic_train$survived) %>%
  accuracy(truth = titanic_train$survived, estimate = .pred_class)
lda_predict

qda_predict <- predict(qda_fit, titanic_train) %>%
  bind_cols(titanic_train$survived) %>%
  accuracy(truth = titanic_train$survived, estimate = .pred_class)
qda_predict

nb_predict <- predict(nb_fit, titanic_train) %>%
  bind_cols(titanic_train$survived) %>%
  accuracy(truth = titanic_train$survived, estimate = .pred_class)
nb_predict
```
In order to compare the predictions and discover which model achieved the highest accuracy on the training data, we can make a table of the accuracy rates.
```{r}
accuracies <- c(log_predict$.estimate, lda_predict$.estimate, 
                qda_predict$.estimate, nb_predict$.estimate)
models <- c("Logistic Regression", "LDA", "QDA", "Naive Bayes")
results <- tibble(accuracies = accuracies, models = models)
results %>% 
  arrange(-accuracies)
```
As can be seen in the table above, the logistic regression model achieved the highest accuracy on the training data with an accuracy of 83.15%.

## Question 10

Fitting the model with the highest training accuracy to the testing data. In this case, the logistic regression model.
```{r}
test_model_acc <- predict(log_fit, titanic_test) %>%
  bind_cols(titanic_test$survived) %>%
  accuracy(truth = titanic_test$survived, estimate = .pred_class)
test_model_acc
```
Fitting the logistic regression model on the testing data yields an accuracy of 80.11%, which is lower than for the training data, but is still rather accurate.

Now, to make a confusion matrix for the testing data and its visualization.
```{r}
augment(log_fit, titanic_test) %>%
  conf_mat(truth = survived, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

Plotting an ROC curve on the testing data and calculating the area under the curve (AUC).
```{r}
test_roc_plot <- augment(log_fit, titanic_test) %>%
  roc_curve(survived, .pred_Yes) %>%
  autoplot()
test_roc_plot

predicted <- predict(log_fit, titanic_test, type = "prob")
test_auc <- auc(titanic_test$survived, predicted)
```




