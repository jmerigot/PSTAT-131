---
title: "Predicting Beautiful Sunsets"
subtitle: "Using Machine Learning Models and Social Media Data to Predict the Aesthetic Quality of a California Sunset"

author: "Jules Merigot"
date: "UCSB Fall 2022"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
    df_print: paged
    theme: sandstone
    highlight: tango
---

# Introduction

The aim of this project is to build a machine learning model to predict how beautiful a sunset will be on any given day in the three biggest cities of the wonderful state of California. I will be using self-aggregated data pulled from a Stanford Computer Science Research Study, the official EPA website, and a weather history API source, all of which will be explained in further detail below along with their respective citations. Throughout this project, I will be implementing multiple machine learning techniques to yield the most accurate model for this binary classification problem. 

## Inspiration and Motive

Have you ever sat down and watched a sunset and thought to yourself, "Wow, that is stunning." Me too. Matter of fact, watching sunsets on the beach here in sunny Santa Barbara account for some of my fondest memories during my 4 years at UCSB. The rush of dopamine as you hurry down the street towards the beach, and the shot of serotonin when you run out onto the sand to discover the work of art that Mother Nature has prepared for you, is indescribable. You feel truly alive, and in my experience, when I watch a beautiful sunset, all of my problems seem to wash away in the massive sea that is the beauty of our planet. 

However, with that beauty can also come the feeling of pure disappointment when you miss a once in a lifetime sunset. It's happened to me, it's happened to you, it's happened to all of us, but never again. In fact, I remember sitting on the deck of my house one day, admiring the sunset, and I thought to myself how sad it would be if I had missed it (seen below).

*insert sunset picture*

Then it occurred to me. What if we could predict when there would be a beautiful sunset? I turned the idea over in my head for a while. Most of us are aware of how difficult it is to predict weather already, so imagine predicting a sunset... But I am not one to give up because of something so trivial. I decided that if I was able to accumulate enough data combining past sunsets and their beauty scores, past air quality data, and meteorological data from those days, I would be able to predict the beauty of a sunset on any given day. While beauty is obviously subjective, the scores on which I will be basing my ratings and predictions on have been thoroughly analyzed, and will be explained later in the Data section. If not for me, then for all my friends, family, and fellow students, I want to be able to notify them of an incoming magical moment. The goal of this project is to help those with limited free time and an appreciation for the pureness of our environment to never miss a beautiful sunset again.

## Data Description

I have myself combined this data into a csv file, binding on specific date and location. The completed dataset includes data on

Relevant links for Data:

https://cs.stanford.edu/~emmap1/sunset_paper.pdf
https://aqs.epa.gov/aqsweb/airdata/download_files.html
https://www.visualcrossing.com/weather/weather-data-services#


```{r setup, include=FALSE}
library(ggplot2)
library(MASS)
library(plyr)
library(dplyr)
library(tidyverse)
library(tidymodels)
library(ISLR)
library(ISLR2)
library(corrplot)
library(ggthemes)
library(discrim)
library(glmnet)
library(poissonreg)
library(corrr)
library(klaR) # for naive bayes
library(finalfit)
library(pROC)
library(usethis)
library(janitor)
tidymodels_prefer()

library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```

## Project Outline



# Exploring Our Data

We will load the data that I have collected and delete some unnecessary variables. Since I assembled this dataset myself, there will be a lot less cleaning tidying required.

## Loading and Exploring the Data

```{r}
# loading the data
sunset_data <- read.csv("C:/Users/jules/OneDrive/Desktop/sunset_quality_data.csv")

# cleaning predictor names
sunset_data <- clean_names(sunset_data)
```

While this data was assembled by hand, there may still be some missing data. This happens often when collecting meteorological data if instruments are not working properly that day, or due to external factors not under our control. So let's check for missing data in our dataset. We'll make a missing plot to better visualize which variables are missing data points.
```{r}
# plot of missing values in the data
sunset_data %>%
  missing_plot()

# the number of missing values in the training data
sum(is.na(sunset_data))
```
Wow! As we can see in our missing values plot, we're missing a lot of data for `windgust`, `solarradiation`, `solarenergy`, `uvindex`, and `severerisk`. With so much missing data in all these predictors, we will be forced to remove them from the dataset entirely because it will otherwise impede on our predictions and cause errors.   
## Tidying the Data

Let's drop the variables with too much missing data. Additionally, some predictors that I originally kept will also not be needed so they will be removed below. We will also change our response variable `good_sunset`, which is currently a binary variable, to be a factor, and reorder the factor so that “Yes” is the second level. "Yes" will mean it was a good sunset, and "No" will be... well, its pretty self-explanatory. So let's do it!
```{r}
drop <- c("i_city_and_date", "missing_days", "date_of_last_change", "datetime", 
          "severerisk", "stations", "sunrise", "description", "city_name", 
          "snow", "snowdepth", "windgust", "solarenergy", "solarradiation",
          "uvindex", "residual_after_controlling_for_time", "state_name")
sunset_data = sunset_data[,!(names(sunset_data) %in% drop)]

# Make good_sunset a factor
sunset_data$good_sunset <- factor(sunset_data$good_sunset, 
                                  labels = c("No", "Yes"))
```

One of our variables `date` which distinguishes the date of a particular sunset, is currently in mm/dd/yyyy format, which is difficult to use in a prediction or classification model, so let's extract the month and year from each value and make them their own predictors. The specific days won't matter as much here, but the particular month will have an impact since it traverses across seasons (November to May). It is commonly known that sunsets tend to be more vivid in color in the winter months, therefore there may be a significant difference in prediction results if a particular sunset's quality was evaluated in December of 2015 compared to one in May of 2016. While we're at it, let's also rename out air quality variables to something shorter so they take up less room in our visual plots later on. Don't worry, I'll explain what each one is in a bit. 
```{r}
dates <- as.POSIXct(sunset_data$date, format = "%m/%d/%Y")

sunset_data <- sunset_data %>%
  add_column("year" = c(format(dates, format="%Y")), .before = 1) %>%
  add_column("month" = c(format(dates, format="%m")), .before = 1) %>%
  plyr::rename(c("co_in_parts_per_million" = "co_in_ppm",
         "no2_in_parts_per_billion" = "no2_in_ppb",
         "so2_in_parts_per_billion" = "so2_in_ppb",
         "ozone_in_parts_per_million" = "ozone_in_ppm"))
```

Now that we've added these columns, let's make them numeric variables so they can be used in our recipe later on without being dummy coded.
```{r}
# make Month and Year numeric variables 
sunset_data$month <- as.numeric(as.character(sunset_data$month))
sunset_data$year <- as.numeric(as.character(sunset_data$year))

sunset_data %>% dim()

sunset_data %>% head()
```

## Describing the Predictors

The variables that I selected for the true data set are as following:  

- `month`: The month that the sunset was recorded (ranges from November 2015 to May 2016) as represented by its respective number (e.g. 11 for November)   
- `year`: The year that the sunset was recorded (either 2015 or 2016)   
- `city`: The city in which the sunset was recorded (either Los Angeles, San Francisco, or San Diego)
- `raw_number_of_posts`: The number of Instagram posts recorded and analyzed for that sunset
- `latitude`: The latitude coordinates of the sunset
- `longitude`: The longitude coordinates of the sunset
- `co_in_ppm`: The carbon monoxide CO) levels in the air on the given day of the sunset, measured in parts per million
- `no2_in_ppm`: The nitrogen dioxide (NO2) levels in the air on the given day of the sunset, measured in parts per million
- `so2_in_ppm`: The sulfur dioxide (SO2) levels in the air on the given day of the sunset, measured in parts per million
- `ozone_in_ppm`: The ozone (O3) levels in the air on the given day of the sunset, measured in parts per million
- `aqi`: The air quality index on the given day of the sunset in the city it took place
- `tempmax`: The maximum recorded temperature on the given day of the sunset in the city in took place
- `tempmin`: The minimum recorded temperature on the given day of the sunset in the city in took place
- `temp`: The average recorded temperature on the given day of the sunset in the city in took place
- `dew`:
- `humidity`:
- `precip`:
- `precipprob`:
- `precipcover`:
- 


## Visual EDA

To get a better idea of the particular distribution of our response variable and our predictors, we'll generate an output variable plot as well a correlation matrix to identify potential correlation between our predictor variables.

### Good Sunset Distribution

One important thing to note before diving deeper into building our models, is to realize that beautiful sunsets are heavily outweighed in occurrence by poorer sunsets. This is expected of course, but can you imagine how awesome it would be if there was a stunning sunset every night!? I would never be able to get any work done! Anyways, below we can see a plot of the distribution of the sunsets and their respective aesthetic quality over the seven months. As a reminder, a "Yes" for a sunset means it was in the top 15% of the most beautiful recorded sunsets.
```{r}
# looking at the distribution of the Good Sunsets
sunset_data %>% 
  ggplot(aes(x = good_sunset)) +
  geom_bar()
```
   
As we can see, while there were less than 100 sunsets that made the top 15%, an overwhelming amount of just slightly more than 450 sunsets were considered to be not as beautiful. This means that once we have built and fit our models, we'll be able to alert our friends of sunsets that are really worth it.

### Variable Correlation Plot

To get an idea of the relationships between our numeric variables, we'll make a correlation matrix and then make a heat map of the correlation of these predictors.
```{r}
# making a correlation matrix and heat map of the predictors
sunset_data %>%
  select(where(is.numeric)) %>%
  cor() %>%
  corrplot()
```
    
Wow those colors almost resemble those present during a sunset, how crazy. I was not too surprised to see so much correlation between my predictors since they encompass most weather and air quality data available, and therefore will tend to be related. However, a few stood out to me. First, the big blue box in the middle represent all the temperature variables which will obviously be strongly correlated in this instance. I was surprised to find how much of a positive correlation the ozone concentration in parts per million had with my various temperature variables. I was also interested to discover that the cloud cover of a given day was negatively correlated with the carbon monoxide and nitrogen dioxide levels in the air.

# Setting Up Models

## Train/Test Split

First, we'll set our seed, and create our initial split of the data to create our training and testing data sets which we will be referencing and using throughout this project.
```{r}
set.seed(8488)

sunset_split <- initial_split(sunset_data, prop=0.75, strata=good_sunset)

sunset_train <- training(sunset_split)
dim(sunset_train)
sunset_test <- testing(sunset_split)
dim(sunset_test)
```
For splitting the data, I chose a proportion of 0.75 because it allows for more training data, while retaining enough data to be tested since there is a limited amount of observations. The training data has 402 observations while the testing data has 136 observations.  

## Building Our Recipe

```{r}
# making the recipe
sunset_recipe <- recipe(good_sunset ~ tempmax + tempmin + humidity +  
                          cloudcover + aqi + precip + 
                          windspeed +  sealevelpressure + month +
                          ozone_in_ppm + visibility, 
                        data=sunset_train) %>%
  step_interact(terms = ~ humidity:cloudcover)
sunset_recipe %>% prep() %>% juice()
```

```{r}
# logistical regression model as an example
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>%
  set_mode("classification")

log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(sunset_recipe)

log_fit <- fit(log_wkflow, sunset_train)
predict(log_fit, new_data = sunset_train, type="prob") #%>% View()

log_fit %>% 
  tidy()

#sunset_train$Good.Sunset %>% table()
```

# Building Prediction Models

## K-Fold Cross-Validation

```{r}
sunset_folds <- vfold_cv(sunset_train, v = 10)
sunset_folds

log_kfold_fit <- fit_resamples(log_wkflow, sunset_folds)

collect_metrics(log_kfold_fit)

#lasso
#standardize predictors
```

Plotting an ROC curve on the testing data and calculating the area under the curve (AUC).
```{r}
roc <- augment(log_fit, sunset_test)

# plotting the ROC curve
roc %>%
  roc_curve(good_sunset, .pred_No) %>%
  autoplot()

# calculating the AUC of the curve
roc %>%
  roc_auc(good_sunset, .pred_No)
```

# Lasso

```{r}
sunset_spec <- multinom_reg(penalty = tune(), mixture = tune()) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

sunset_workflow <- workflow() %>% 
  add_recipe(sunset_recipe) %>% 
  add_model(sunset_spec)

pen_mix_grid <- grid_regular(penalty(range = c(-5, 5)), mixture(range = c(0,1)), levels = 10)
pen_mix_grid
```

Let's fit the models to our folded data using `tune_grid()`.
```{r, warning=FALSE, results='hide'}
tune_res <- tune_grid(
  sunset_workflow,
  resamples = sunset_folds, 
  grid = pen_mix_grid
)

autoplot(tune_res)
```

Let's use `select_best()` to choose the model that has the optimal `roc_auc`.
```{r}
collect_metrics(tune_res)

best_penalty <- select_best(tune_res, metric = "roc_auc")
best_penalty
```

Then we'll use `finalize_workflow()`, `fit()`, and `augment()` to fit the model to the training set and evaluate its performance on the testing set.
```{r, warning=FALSE}
lasso_final <- finalize_workflow(sunset_workflow, best_penalty)

lasso_final_fit <- fit(lasso_final, data = sunset_train)

augment(lasso_final_fit, new_data = sunset_test) %>%
  accuracy(truth = good_sunset, estimate = .pred_class)
```

Almost there! Now let's calculate the overall ROC AUC on the testing set.
```{r}
roc <- augment(lasso_final_fit, sunset_test)

roc %>%
  roc_auc(good_sunset, .pred_No)
```

Then we'll create plots of the ROC curve. 
```{r}
roc %>%
  roc_curve(good_sunset, .pred_No) %>%
  autoplot()
```

