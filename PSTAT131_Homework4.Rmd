---
title: "Homework 4"
author: "Jules Merigot (8488256)"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: pdf_document
---

```{r setup, echo=FALSE, include=FALSE}
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(ISLR)
library(ISLR2)
library(corrplot)
library(ggthemes)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR) # for naive bayes
library(finalfit)
library(pROC)
tidymodels_prefer()

library(knitr)
# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(fig.width=7, fig.height=5)
options(digits = 4)

## indents are for indenting r code as formatted text
## They may need to be adjusted depending on your OS
# if your output looks odd, increase or decrease indent
indent1 = '    '
indent2 = '        '
indent3 = '            '
```

\begin{center}
PSTAT 131/231 Statistical Machine Learning - Fall 2022
\end{center}

# Resampling

Before we get started, let's first load the titanic data and change the `survived` and `pclass` variables to factors. 
```{r, results="hide"}
# loading the data
titanic_data <- read.csv(file = "C:/Users/jules/OneDrive/Desktop/homework-4/data/titanic.csv")
head(titanic_data)

titanic_data$survived <- factor(titanic_data$survived, labels = c("Yes", "No"))
titanic_data$pclass <- factor(titanic_data$pclass)

str(titanic_data)
```


## Question 1

Let's set a seed, and randomly split the data to create a training and testing set. We'll choose appropriate proportions and stratify on the outcome variable, `survived`.
```{r}
# setting the seed
set.seed(8488)

titanic_split <- initial_split(titanic_data, prop=0.70, strata=survived)

titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
```
For splitting the data, I chose a proportion of 0.70 because it allows for more training data, while retaining enough data to be tested since there is a limited amount of observations. The training data has 623 observations while the testing data has 268 observations.  
   

Next, let's make our recipe, while accounting for missing values and creating the proper interactions.
```{r}
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, 
                         data=titanic_train) %>%
  step_impute_linear(age, impute_with = imp_vars(all_predictors())) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(terms = ~ sex_male:fare + age:fare) 

titanic_recipe %>% prep() %>% juice()
```

## Question 2

Now, we fold the training data using k-fold cross-validation, with k=10 (represented by v in the ISLR package terminology).
```{r}
titanic_folds <- vfold_cv(titanic_train, v = 10)
titanic_folds
```

## Question 3

In Question 2 above, we are randomly splitting our titanic training dataset into 10 groups or folds, as denoted by the parameter k, of approximately equal size. Essentially, k-fold cross validation is a statistical method or resampling procedure used to evaluate the skill of machine learning models on a limited dataset. When fitting the model, the model is trained on (k-1) folds and tested on the one left out fold. This process is then repeated k times until the model is trained and tested on all the folds. This method allows for users to fit and test models on various groups of data, as designated by the k=10 folds, which is easy to implement and thus results in skill estimates that generally have a lower bias than other methods. If instead we used a resampling method on the entire training set, it would be considered the Validation Set Approach, which purely uses a training dataset to test and fit the model, and then a testing dataset to fit the final model.

## Question 4

We will now set up workflows for 3 models:    

A logistic regression with the glm engine.
```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_recipe)
```

A linear discriminant analysis with the MASS engine.
```{r}
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titanic_recipe)
```

A quadratic discriminant analysis with the MASS engine.
```{r}
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)
```

With 10 folds and with the 3 models created above, we will be fitting a total of 30 models to the data.

## Question 5

We will now fit each of the created models to the folded data.
```{r}
log_fit <- fit_resamples(log_wkflow, titanic_folds)

lda_fit <- fit_resamples(lda_wkflow, titanic_folds)

qda_fit <- fit_resamples(qda_wkflow, titanic_folds)
```

## Question 6

We will now use `collect_metrics()` to print the mean and standard errors of the performance metric accuracy across all folds for each of the three models.
```{r}
collect_metrics(log_fit)
collect_metrics(lda_fit)
collect_metrics(qda_fit)
```
The fitted model that performed the best is the linear discriminant analysis model. While this model did not have the highest mean accuracy, 0.8072 compared to the 0.8152 mean accuracy of the logistic regression model, it is considered more accurate relative to the standard error accuracy. The standard error of the linear discriminant analysis model is 0.01263, while the standard error of the logistic regression model is higher at 0.01782. Since its standard error is much lower than that of the logistic regression model, even though the linear discriminant analysis model has a slightly lower mean accuracy, it is still considered the more accurate fitted model in this case.

## Question 7

Now that we’ve chosen the linear discriminant analysis model as the most accurate, let's fit the model to the entire training dataset.
```{r}
final_log_fit <- fit(log_wkflow, titanic_train)
final_log_fit
```

## Question 8

Finally, with our fitted model, we will use `predict()`, `bind_cols()`, and `accuracy()` to assess our model’s performance on the testing data!
```{r}
test_model_acc <- predict(final_log_fit, titanic_test) %>%
  bind_cols(titanic_test$survived) %>%
  accuracy(truth = titanic_test$survived, estimate = .pred_class)
test_model_acc

Test_model_acc <- predict(final_log_fit, titanic_test, type="class") %>%
  bind_cols(titanic_test %>% select(survived)) %>%
  accuracy(truth = survived, estimate = .pred_class)
Test_model_acc
```
As we can see above, our model’s testing accuracy is slightly lower than our average accuracy across the folds. Our mean accuracy across all folds for the linear discriminant analysis is 0.8072, while our model's testing accuracy is 0.7873. This is normal and expected since the model was optimized for the training data and the folds in its earlier stages, therefore it will tend to have a lower accuracy when applied to the testing data.   

Overall, this model is rather accurate with an accuracy nearing 80%. The linear discriminant analysis model is definitely the better choice in this case.
